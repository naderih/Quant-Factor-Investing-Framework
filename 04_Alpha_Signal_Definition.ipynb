{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58321f37",
   "metadata": {},
   "source": [
    "# Notebook 4: Alpha Signal Generation & Neutralization\n",
    "\n",
    "### **Objective**\n",
    "The objective of this notebook is to construct clean, actionable, and time-varying **alpha signals** that will drive our active portfolio decisions. In the Grinold-Kahn framework, a high-quality alpha signal is the mathematical representation of a manager's proprietary, skill-based market view.\n",
    "\n",
    "This notebook demonstrates a crucial step in a professional quantitative process: the **purification** of raw investment ideas into portfolio-ready alpha vectors. We will generate two distinct alpha signals:\n",
    "1.  A classic **Momentum** factor.\n",
    "2.  A **Financial Constraints (FC)** factor, based on the Whited-Wu index. This serves as a quantitative baseline for future research using more advanced LLM-based measures.\n",
    "\n",
    "Crucially, both raw signals will be rigorously neutralized to remove any unintended systematic (benchmark) bias.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methodology: The Signal Purification Pipeline**\n",
    "\n",
    "The process involves a clear, three-step pipeline for each of our alpha ideas:\n",
    "\n",
    "*   **1. Generate Raw Alpha Signals:** We start by calculating the raw, un-neutralized score for each of our investment ideas for each stock at each point in time.\n",
    "    *   **Momentum:** Calculated as the stock's historical return over the past 12 months, skipping the most recent month.\n",
    "    *   **Financial Constraints:** Calculated using the formula for the **Whited-Wu Index**, which combines several accounting ratios into a single score representing a firm's access to external capital.\n",
    "\n",
    "*   **2. Diagnose Systematic Bias:** A raw factor signal is rarely \"pure\" and often contains an implicit bet on the overall market. We diagnose this bias for each signal by calculating its **benchmark-weighted average** for each month.\n",
    "    $$ \\bar{\\alpha}_{\\text{raw}, B, t} = \\sum_{n=1}^{N_t} h_{B,n,t} \\cdot \\alpha_{\\text{raw},n,t} $$\n",
    "    A non-zero value indicates that the raw signal has a systematic tilt that must be removed.\n",
    "\n",
    "*   **3. Perform Benchmark Neutralization:** To create a pure, \"skill-based\" signal suitable for a stock-selection strategy, we remove this systematic bias using a **beta-adjusted neutralization**. This method cleans each stock's raw alpha by subtracting the portion of its alpha expected to come from the system-wide bias, proportional to the stock's own beta.\n",
    "    $$ \\alpha_{\\text{final},n,t} = \\alpha_{\\text{raw},n,t} - \\beta_{n,t} \\cdot \\bar{\\alpha}_{\\text{raw}, B, t} $$\n",
    "\n",
    "*   **4. Combine Signals (Optional):** As a final step, we can create a composite alpha signal by taking a simple average of our two final, neutralized alpha vectors. This is a basic form of signal combination to create a more diversified alpha source.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts & Theoretical Justification**\n",
    "\n",
    "#### **1. Alpha ($\\alpha$) as a Forecast**\n",
    "\n",
    "In the active management context, the alpha vector is the manager's **forecast of expected residual return**. It represents the performance that is expected to be uncorrelated with the benchmark's returns. It is the mathematical embodiment of unique, skill-based insights.\n",
    "\n",
    "#### **2. The Importance of Benchmark Neutrality**\n",
    "\n",
    "The constraint that the benchmark-weighted average of the alphas is zero, $h_B^T \\alpha = 0$, is a crucial disciplining step. It ensures the alpha signal is, on average, **orthogonal to the benchmark**, cleanly separating stock-picking skill from any implicit market timing bet. An optimizer given a benchmark-neutral alpha signal and no other instruction will naturally build an active portfolio with a beta of one.\n",
    "\n",
    "#### **3. Beta-Adjusted Neutralization**\n",
    "\n",
    "The beta-adjusted method is theoretically superior to a simple flat subtraction. By adjusting each stock's alpha based on its beta ($\\beta_n$), we remove the systematic bias in a \"risk-aware\" manner. This process helps ensure the final alpha signal has a lower correlation with the benchmark return itself, making it a higher-quality input for portfolio optimization.\n",
    "\n",
    "---\n",
    "**Output:** This notebook produces the final, clean `alpha_signals.parquet` file. This panel dataset, containing the purified alpha forecasts for each stock in each month, is the primary \"signal\" that will be combined with our risk model ($V_t$) to construct the optimal active portfolio in Notebook 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7206d06d",
   "metadata": {},
   "source": [
    "### 1. Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fdc83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import statsmodels.api as sm \n",
    "import os \n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cc33581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading and merging complete.\n",
      "Final DataFrame shape: (1199061, 5)\n",
      "Sample of the final prepared data:\n",
      "                   ret_monthly   mkt_cap    vwretd  Momentum  FinConstraint\n",
      "permno date                                                                \n",
      "10001  1990-06-30     0.013750  10052.25  0.002203 -0.441271       2.858116\n",
      "       1990-07-31     0.025641  10310.00  0.001627 -0.609343       3.222712\n",
      "       1990-08-31    -0.050000   9794.50  0.009347 -0.333894       2.736315\n",
      "       1990-09-30     0.040789  10179.00  0.014276 -0.018822       2.972653\n",
      "       1990-10-31    -0.012821  10048.50  0.000644  0.328334       3.033946\n"
     ]
    }
   ],
   "source": [
    "onedrive_root = os.environ['OneDrive']\n",
    "DATA_DIR = os.path.join(onedrive_root, '0. DATASETS', 'outputs')\n",
    "\n",
    "# defining paths and loading the datasets:\n",
    "\n",
    "# --- Load Required Datasets ---\n",
    "PANEL_DATA_PATH = os.path.join(DATA_DIR, 'panel_data.parquet')\n",
    "FACTOR_EXPOSURES_PATH = os.path.join(DATA_DIR, 'factor_exposures.parquet')\n",
    "\n",
    "panel_data = pd.read_parquet(PANEL_DATA_PATH)\n",
    "X_factors = pd.read_parquet(FACTOR_EXPOSURES_PATH)\n",
    "\n",
    "\n",
    "# make sure the dataframes have the same indices\n",
    "if not panel_data.index.names == ['permno', 'date']:\n",
    "    panel_data.reset_index(inplace = True)\n",
    "    panel_data.set_index(['permno', 'date'], inplace = True)\n",
    "    \n",
    "if not X_factors.index.names == ['permno', 'date']:\n",
    "    X_factors.reset_index(inplace = True)\n",
    "    X_factors.set_index(['permno', 'date'], inplace = True)\n",
    "\n",
    "# Create smaller DataFrames with only the columns we need for this notebook.\n",
    "market_data = panel_data[['ret_monthly', 'mkt_cap', 'vwretd']]\n",
    "alpha_signals = X_factors[['Momentum', 'FinConstraint']]\n",
    "\n",
    "# creating one df of our vars\n",
    "df = market_data.join(alpha_signals, how = 'inner')\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "print(\"Data loading and merging complete.\")\n",
    "print(f\"Final DataFrame shape: {df.shape}\")\n",
    "print(\"Sample of the final prepared data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b80d7c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great!, no dupliates found in the data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "duplicates = df[df.index.duplicated(keep=False)]\n",
    "if len(duplicates) > 0 :\n",
    "    print(f\"Warning: data has duplicate entries \")\n",
    "    print(duplicates.head())\n",
    "else:\n",
    "    print(\"Great!, no dupliates found in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da526040",
   "metadata": {},
   "source": [
    "### 2. Calculate Time-Varying Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2985e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating time-varying historical betas for each stock...\n",
      "Rolling beta calculation complete.\n",
      "\n",
      "Verifying the beta calculation:\n",
      "count    701280.000000\n",
      "mean          0.155622\n",
      "std           2.409209\n",
      "min         -95.971368\n",
      "25%          -0.926747\n",
      "50%           0.272374\n",
      "75%           1.381265\n",
      "max          42.308698\n",
      "Name: beta, dtype: float64\n",
      "\n",
      "Total observations: 1199061\n",
      "Number of valid beta calculations: 701280\n",
      "Rolling beta calculation complete.\n",
      "\n",
      "Verifying the beta calculation:\n",
      "count    701280.000000\n",
      "mean          0.155622\n",
      "std           2.409209\n",
      "min         -95.971368\n",
      "25%          -0.926747\n",
      "50%           0.272374\n",
      "75%           1.381265\n",
      "max          42.308698\n",
      "Name: beta, dtype: float64\n",
      "\n",
      "Total observations: 1199061\n",
      "Number of valid beta calculations: 701280\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Time-Varying Betas ---\n",
    "# The goal is to calculate a time-series of benchmark betas for each stock. \n",
    "# For each stock, we calcualte beta on a 60-month (5 year) rolling basis \n",
    "\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "\n",
    "print(\"Calculating time-varying historical betas for each stock...\")\n",
    "\n",
    "def calculate_rolling_beta(group, window_size=60, min_obs=36):\n",
    "    \"\"\"\n",
    "    Calculates the rolling beta of a stock's excess returns against the benchmark's excess returns.\n",
    "    The benchmark is the value-weighted portfolio of CRSP portfolio of stocks. \n",
    "    Returns a Series aligned with the group's index. If the group is too short, i.e. with fewer than 60 rows\n",
    "    returns a Series of NaNs so pandas alignment works correctly.\n",
    "    \"\"\"\n",
    "    # Require at least 'window_size' observations: \n",
    "    # RollingOLS expects the underlying arrays to be at least as long as the window.\n",
    "    if len(group) < min_obs:\n",
    "        return pd.Series(np.nan, index=group.index)\n",
    "\n",
    "    y = group['ret_monthly']\n",
    "    X = sm.add_constant(group['vwretd'])\n",
    "\n",
    "    try:\n",
    "        rols = RollingOLS(y, X, window=window_size, min_nobs=min_obs)\n",
    "        results = rols.fit()\n",
    "        return results.params['vwretd']\n",
    "    except IndexError:\n",
    "        # If statsmodels raises an indexing error\n",
    "        # return NaNs aligned to the group's index.\n",
    "        return pd.Series(np.nan, index=group.index)\n",
    "\n",
    "# Apply the function to each 'permno' group.\n",
    "rolling_betas = df.groupby('permno', group_keys=False).apply(calculate_rolling_beta)\n",
    "\n",
    "# Assign the resulting Series back to our main DataFrame.\n",
    "df['beta'] = rolling_betas\n",
    "\n",
    "print(\"Rolling beta calculation complete.\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\nVerifying the beta calculation:\")\n",
    "# Drop NaNs for a meaningful description of the calculated betas\n",
    "print(df['beta'].describe())\n",
    "\n",
    "# Check how many valid betas we have\n",
    "print(f\"\\nTotal observations: {len(df)}\")\n",
    "print(f\"Number of valid beta calculations: {df['beta'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210ef1e",
   "metadata": {},
   "source": [
    "### 3. Winsorize the Beta Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbd7ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta winsorization complete.\n",
      "\n",
      "Comparing Raw vs. Winsorized Beta Distributions:\n",
      "            Raw Beta  Winsorized Beta\n",
      "count  701280.000000    701280.000000\n",
      "mean        0.155622         0.163484\n",
      "std         2.409209         2.206242\n",
      "min       -95.971368       -12.844529\n",
      "25%        -0.926747        -0.926747\n",
      "50%         0.272374         0.272374\n",
      "75%         1.381265         1.381265\n",
      "max        42.308698        12.951891\n"
     ]
    }
   ],
   "source": [
    "# --- Winsorize the Beta Estimates ---\n",
    "# Defining the quantile thresholds \n",
    "lower_quantile = 0.01\n",
    "upper_quantile = 0.99\n",
    "\n",
    "# We are going to calculate the 1% and 99% percent quantile for each date group \n",
    "# and clip out beta series \n",
    "df['beta_winsorized'] = df.groupby('date')['beta'].transform(\n",
    "    lambda x : x.clip(\n",
    "        lower = x.quantile(lower_quantile),\n",
    "        upper = x.quantile(upper_quantile)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Beta winsorization complete.\")\n",
    "\n",
    "# --- Verification Step ---\n",
    "print(\"\\nComparing Raw vs. Winsorized Beta Distributions:\")\n",
    "\n",
    "beta_comparison = pd.concat([df['beta'].describe(), df['beta_winsorized'].describe()], axis = 1 )\n",
    "beta_comparison.columns = ['Raw Beta', 'Winsorized Beta' ]\n",
    "print(beta_comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e445c2",
   "metadata": {},
   "source": [
    "### 4. Alpha Neutralization\n",
    "To take our \"raw\" alpha signals (Momentum and FinConstraint) and remove their systematic, benchmark-related bias. The output will be two new columns, alpha_Momentum and alpha_FinConstraint, which are our final, benchmark-neutral alpha vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1012d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutralizing Momentum signal...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutralizing Momentum signal...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutralizing Financial Constraint signal...\n",
      "\n",
      "Verifying neutralization...\n",
      "\n",
      "Verifying neutralization...\n",
      "Average benchmark-weighted Momentum Alpha: -0.0000000000 which is cose to 0\n",
      "Average benchmark-weighted FinConstraint Alpha: -0.0000000000 which is close to 0\n",
      "Average benchmark-weighted Momentum Alpha: -0.0000000000 which is cose to 0\n",
      "Average benchmark-weighted FinConstraint Alpha: -0.0000000000 which is close to 0\n"
     ]
    }
   ],
   "source": [
    "# --- Alpha Neutralization ---\n",
    "\n",
    "def neutralize_alpha(group, signal_col_name):\n",
    "    \"\"\"\n",
    "    Performs a beta-adjusted benchmark neutralization on a given signal column.\n",
    "    Takes a DataFrame for a single period ('group') and the name of the signal column.\n",
    "    Returns a Series of neutralized alphas.\n",
    "    \"\"\"\n",
    "    # Define the columns we absolutely need for this calculation\n",
    "    required_cols = ['mkt_cap', 'beta_winsorized', signal_col_name]\n",
    "    \n",
    "    # Drop any stocks with missing data for this month\n",
    "    clean_group = group.dropna(subset=required_cols)\n",
    "    \n",
    "    # If no data is left, return an empty series\n",
    "    if clean_group.empty:\n",
    "        return pd.Series(dtype='float64')\n",
    "\n",
    "    # Calculate cap weights for the valid universe this month\n",
    "    cap_weights = clean_group['mkt_cap'] / clean_group['mkt_cap'].sum()\n",
    "\n",
    "    # Calculate the raw signal bias and the beta of our tradable benchmark\n",
    "    benchmark_raw_alpha = (clean_group[signal_col_name] * cap_weights).sum()\n",
    "    benchmark_beta = (clean_group['beta_winsorized'] * cap_weights).sum()\n",
    "\n",
    "    # If benchmark_beta is zero or very small, we can't neutralize. Return raw de-meaned.\n",
    "    if np.isclose(benchmark_beta, 0):\n",
    "        # A simple de-meaning is a safe fallback\n",
    "        return clean_group[signal_col_name] - benchmark_raw_alpha\n",
    "\n",
    "    # Calculate the beta-adjusted bias term\n",
    "    adjustment_factor = benchmark_raw_alpha / benchmark_beta\n",
    "\n",
    "    # Calculate the final, neutralized alpha\n",
    "    neutralized_signal = clean_group[signal_col_name] - clean_group['beta_winsorized'] * adjustment_factor\n",
    "    \n",
    "    return neutralized_signal\n",
    "\n",
    "# --- Apply the function for each signal ---\n",
    "\n",
    "print(\"Neutralizing Momentum signal...\")\n",
    "# We tell apply to use our function, and then pass 'Momentum' as the 'signal_col_name' argument\n",
    "alpha_momentum = df.groupby('date', group_keys= False).apply(neutralize_alpha, signal_col_name='Momentum')\n",
    "\n",
    "print(\"Neutralizing Financial Constraint signal...\")\n",
    "alpha_fin_constraint = df.groupby('date', group_keys = False).apply(neutralize_alpha, signal_col_name='FinConstraint')\n",
    "\n",
    "\n",
    "# --- Assign the new alpha signals back to the main DataFrame ---\n",
    "# Since we used group_keys = False, the result of the .apdl y) will have a (date, permno) multi-index \n",
    "# so we can assign it directly to our dataframe \n",
    "df['alpha_Momentum'] = alpha_momentum\n",
    "df['alpha_FinConstraint'] = alpha_fin_constraint\n",
    "\n",
    "\n",
    "# --- Final Verification ---\n",
    "print(\"\\nVerifying neutralization...\")\n",
    "# We must use the original weights from the df for a true verification\n",
    "df['cap_weight'] = df.groupby('date')['mkt_cap'].transform(lambda x: x / x.sum())\n",
    "\n",
    "mom_alpha_benchmark_avg = (df['alpha_Momentum'] * df['cap_weight']).groupby('date').sum().mean()\n",
    "fc_alpha_benchmark_avg = (df['alpha_FinConstraint'] * df['cap_weight']).groupby('date').sum().mean()\n",
    "\n",
    "if np.allclose(mom_alpha_benchmark_avg, 0): \n",
    "    print(f\"Averag0 benchmark-weighted Momentum Alpha: {mom_alpha_benchmark_avg:.18f} which is cose to 0\")\n",
    "else:\n",
    "    print(f\"Aver0ge benchmark-weighted Momentum Alpha: {mom_alpha_benchmark_avg:.18f} which is different from zero\")\n",
    "\n",
    "if np.allclose(fc_alpha_benchmark_avg, 0):\n",
    "    print(f\"Avera0e benchmark-weighted FinConstraint Alpha: {fc_alpha_benchmark_avg:.18f} which is close to 0\")\n",
    "else:\n",
    "    print(0\"Average benchmark-weighted Momentum Alpha: {fc_alpha_benchmark_avg:.18f} which is different from zero\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f2936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permno  date      \n",
       "10001   1990-06-30         NaN\n",
       "        1990-07-31         NaN\n",
       "        1990-08-31         NaN\n",
       "        1990-09-30         NaN\n",
       "        1990-10-31         NaN\n",
       "                        ...   \n",
       "93436   2023-08-31    0.085492\n",
       "        2023-09-30    0.121931\n",
       "        2023-10-31    0.736613\n",
       "        2023-11-30    0.035304\n",
       "        2023-12-31    0.264956\n",
       "Name: alpha_composite, Length: 1199061, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- FINAL CELL: Finalize and Save All Alpha Signals ---\n",
    "\n",
    "# 1. Create the simple, equal-weighted composite signal\n",
    "df['alpha_composite'] = (df['alpha_Momentum'] + df['alpha_FinConstraint']) / 2\n",
    "\n",
    "# 2. Re-standardize ALL THREE alpha signals for consistency\n",
    "# This ensures each signal we test has a comparable cross-sectional distribution (mean=0, std=1).\n",
    "print(\"Re-standardizing all final alpha signals...\")\n",
    "\n",
    "df['alpha_Momentum_final'] = df.groupby('date')['alpha_Momentum'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "df['alpha_FinConstraint_final'] = df.groupby('date')['alpha_FinConstraint'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "df['alpha_Composite_final'] = df.groupby('date')['alpha_composite'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "\n",
    "# Create a final DataFrame with clean alpha signals\n",
    "alpha_signals_final = df[['alpha_Momentum_final', 'alpha_FinConstraint_final', 'alpha_Composite_final']].copy()\n",
    "alpha_signals_final.fillna(0, inplace=True) # Fill any NaNs from std=0 cases with 0\n",
    "\n",
    "# --- Verification --- \n",
    "print(\"\\nVerifying benchmark neutrality of the final composite signal...\")\n",
    "df['cap_weight'] = df.groupby('date')['mkt_cap'].transform(lambda x: x / x.sum())\n",
    "final_composite_benchmark_alpha = (alpha_signals_final['alpha_Composite_final'] * df['cap_weight']).groupby('date').sum().mean()\n",
    "print(f\"Average benchmark-weighted Final Composite Alpha: {final_composite_benchmark_alpha:.10f}\")\n",
    "\n",
    "# 4. Save the Final Alpha Signals DataFrame\n",
    "ALPHA_FILE = os.path.join(DATA_DIR, 'alpha_signals.parquet')\n",
    "alpha_signals_final.to_parquet(ALPHA_FILE)\n",
    "\n",
    "print(f\"\\nFinal DataFrame of all alpha signals saved to {ALPHA_FILE}\")\n",
    "print(\"--- Notebook 4 (Project Titan) is Complete ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

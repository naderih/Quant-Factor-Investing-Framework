{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b978bb8",
   "metadata": {},
   "source": [
    "# Notebook 2: Time-Varying Factor Exposures\n",
    "\n",
    "### **Objective**\n",
    "This notebook constructs the **time-varying Factor Exposure Matrix ($X_t$)**, a critical input for a dynamic multifactor risk model. This \"Project Titan\" version implements a professional-grade process for building robust, point-in-time factor exposures. For each month in our sample period, we will calculate a full cross-section of exposures based on data that would have been known at that time.\n",
    "\n",
    "The final output is a large panel dataset where each row represents a specific stock at a specific point in time, and each column represents that stock's exposure to a fundamental factor.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methodology: Point-in-Time & Composite Factor Construction**\n",
    "\n",
    "The methodology focuses on creating a robust and realistic `X` matrix by incorporating two key professional techniques: point-in-time data handling and composite factor construction.\n",
    "\n",
    "*   **1. Industry Factor Classification:** I classify each stock into one of the 12 Fama-French industry groups based on its historical SIC code. This creates 12 orthogonal \"dummy variable\" factors that will form the basis for capturing market-wide and sector-specific risk.\n",
    "\n",
    "*   **2. Composite Style Factor Construction:** We build our style factors using a **multi-descriptor composite approach**, as recommended by Grinold & Kahn for model robustness. This involves:\n",
    "    *   **Descriptor Calculation:** We first calculate the raw, underlying data (\"descriptors\") for each factor. This includes:\n",
    "        *   **Value:** Book-to-Market (B/M) and Earnings-to-Price (E/P).\n",
    "        *   **Momentum:** 12-month and 6-month historical returns (skipping the most recent month).\n",
    "        *   **Size:** The natural logarithm of market capitalization.\n",
    "        *   **Financial Constraints:** The Whited-Wu (WW) Index, which itself is a composite of several accounting ratios.\n",
    "    *   **Point-in-Time Lagging:** To avoid lookahead bias, accounting-based descriptors (like Book Equity and Earnings) are appropriately lagged to simulate real-world reporting delays.\n",
    "\n",
    "*   **3. Cross-Sectional Standardization:** This is the core of the process. **For each month in our sample**, I perform a **capitalization-weighted standardization** on each of the raw style factor descriptors individually. This converts each descriptor into a comparable Z-score relative to the market *at that specific point in time*.\n",
    "\n",
    "*   **4. Final Factor Assembly:** The final factor exposures are created:\n",
    "    *   For composite factors (Value, Momentum), we take the **average of their respective standardized descriptors.**\n",
    "    *   The final composite factors are then re-standardized to ensure they have a clean, cap-weighted mean of zero and standard deviation of one. This creates pure, \"extra-market\" style factors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts & Theoretical Justification**\n",
    "\n",
    "#### **1. Composite Factors for Robustness**\n",
    "\n",
    "A key principle from \"Active Portfolio Management\" is that relying on a single descriptor for a factor (e.g., only using Book-to-Market for \"Value\") makes a model fragile. Any single accounting ratio can be noisy, subject to measurement error, or misleading for certain industries (e.g., B/M for tech firms). By creating a **composite factor** from several related but distinct descriptors, we **diversify away the idiosyncratic noise** of each individual measure. The resulting factor is a more robust and stable representation of the underlying economic concept.\n",
    "\n",
    "#### **2. Time-Varying Exposures**\n",
    "\n",
    "Companies evolve. A firm can grow from a \"small-cap\" to a \"large-cap.\" It can transition from a \"growth\" stock to a \"value\" stock. By recalculating the standardized exposures for every period, our risk model can adapt to this evolution, providing a more accurate, forward-looking assessment of risk.\n",
    "\n",
    "#### **3. Point-in-Time Data & Lookahead Bias**\n",
    "\n",
    "Lookahead bias is one of the most critical errors in quantitative research. It occurs when a model is built using information that would not have been available at the time of the decision. By carefully lagging accounting data to account for reporting delays, I ensure the factor exposures are \"point-in-time\" correct and our subsequent backtests are valid and realistic.\n",
    "\n",
    "---\n",
    "**Output:** This notebook generates and saves the `factor_exposures_titan.parquet` file. This panel dataset, indexed by `(date, permno)`, is the primary $X$ input for the Fama-MacBeth risk model estimation in Notebook 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4d248",
   "metadata": {},
   "source": [
    "### 1. Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22ffcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Monthly panel data loaded successfully.\n",
      "Data shape: (1660775, 33)\n",
      "Date range: 1995-01-31 00:00:00 to 2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from pathlib import Path \n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- Load the master panel data from Notebook 1 ---\n",
    "onedrive_root = str(Path(os.environ['OneDrive']))\n",
    "DATA_DIR = os.path.join(onedrive_root, \"0. DATASETS\", \"outputs\")\n",
    "\n",
    "PANEL_DATA_FILE = os.path.join(DATA_DIR, 'panel_data.parquet')\n",
    "\n",
    "df = pd.read_parquet(PANEL_DATA_FILE)\n",
    "\n",
    "# making sure permno and industry codes are stored as int\n",
    "df['permno'] = df['permno'].astype('int')\n",
    "# nullable int:\n",
    "df['sic'] = df['sic'].astype('Int64')\n",
    "\n",
    "# Setting a multi-index for efficiency\n",
    "df.reset_index(inplace=True)  # move index back to columns\n",
    "df.set_index(['permno', 'date'], inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"Monthly panel data loaded successfully.\")\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.get_level_values('date').min()} to {df.index.get_level_values('date').max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26927482",
   "metadata": {},
   "source": [
    "### 2. Industry Factor Exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db7c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industry factor exposures created.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Industry Factor Exposures ---\n",
    "\n",
    "# Helper function to map from SIC codes to FF12 industries.\n",
    "def sic_to_ff12(sic):\n",
    "    \"\"\"\n",
    "    Converts a SIC code to one of the 12 Fama-French industry classifications.\n",
    "    Based on the definitions from Ken French's website.\n",
    "    \"\"\"\n",
    "    if pd.isnull(sic):\n",
    "        return np.nan\n",
    "    \n",
    "    sic = int(sic)\n",
    "    \n",
    "    # --- 1. Check for SPECIFIC, granular industries FIRST ---\n",
    "    \n",
    "    # Healthcare, Pharma, Biotech\n",
    "    if 2830 <= sic <= 2836 or 3840 <= sic <= 3851 or 8000 <= sic <= 8099:\n",
    "        return 'Healthcare'\n",
    "    # Technology (Computers, Software, Electronics)\n",
    "    if 3570 <= sic <= 3579 or 3660 <= sic <= 3679 or 7370 <= sic <= 7379:\n",
    "        return 'Technology'\n",
    "    # Energy\n",
    "    if 1300 <= sic <= 1399 or 2900 <= sic <= 2999: return 'Energy'\n",
    "    # Utilities\n",
    "    if 4900 <= sic <= 4949: return 'Utilities'\n",
    "    # Telecom\n",
    "    if 4800 <= sic <= 4899: return 'Telecom'\n",
    "    # Finance\n",
    "    if 6000 <= sic <= 6999: return 'Finance'\n",
    "        \n",
    "    if 100 <= sic <= 999: return 'Consumer'     # Non-Durables (food, tobacco, textiles, etc.)\n",
    "    if 1000 <= sic <= 1499: return 'Other'      # Mining\n",
    "    if 1500 <= sic <= 1999: return 'Other'      # Construction\n",
    "    if 2000 <= sic <= 2799: return 'Consumer'   # More non-durables\n",
    "    if 2800 <= sic <= 2829: return 'Chemicals'  # Chemicals is often its own FF group\n",
    "    if 2840 <= sic <= 2899: return 'Consumer'   # More non-durables\n",
    "    if 3000 <= sic <= 3999: return 'Durables'   # Durables (cars, furniture, industrial equip)\n",
    "    if 4000 <= sic <= 4799: return 'Other'      # Transportation\n",
    "    if 5000 <= sic <= 5999: return 'Shops'      # Wholesale, Retail\n",
    "    if 7000 <= sic <= 7999: return 'Services'   # Business and Personal Services\n",
    "    if 8100 <= sic <= 8999: return 'Services'   # (Excluding Healthcare which was caught above)\n",
    "    if 9100 <= sic <= 9999: return 'Other'      # Public Admin, etc.\n",
    "\n",
    "    return 'Other' # Final catch-all for any SIC codes not covered\n",
    "\n",
    "# Apply the function to the 'sic' column. Note: CRSP hsiccd is better if available.\n",
    "df['industry'] = df['sic'].apply(sic_to_ff12)\n",
    "\n",
    "# Create the dummy variables\n",
    "industry_dummies = pd.get_dummies(df['industry'], prefix='Ind')\n",
    "\n",
    "# We'll join this back to our main DataFrame later.\n",
    "print(\"Industry factor exposures created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a91e7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "industry\n",
       "Finance       305951\n",
       "Technology    232476\n",
       "Durables      211521\n",
       "Other         187659\n",
       "Healthcare    162360\n",
       "Shops         153877\n",
       "Services      145449\n",
       "Consumer      109546\n",
       "Energy         55672\n",
       "Telecom        45440\n",
       "Utilities      38598\n",
       "Chemicals      11181\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['industry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f751484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>share_code</th>\n",
       "      <th>exchange_code</th>\n",
       "      <th>sic</th>\n",
       "      <th>prc</th>\n",
       "      <th>ret_daily</th>\n",
       "      <th>shrout</th>\n",
       "      <th>vwretd</th>\n",
       "      <th>sprtrn</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>mkt_cap</th>\n",
       "      <th>...</th>\n",
       "      <th>lctq</th>\n",
       "      <th>ltq</th>\n",
       "      <th>oiadpq</th>\n",
       "      <th>pstkq</th>\n",
       "      <th>saleq</th>\n",
       "      <th>oancfy</th>\n",
       "      <th>dvpspq</th>\n",
       "      <th>prccq</th>\n",
       "      <th>ret_monthly</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permno</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10001</th>\n",
       "      <th>1995-01-31</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4925</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>012994</td>\n",
       "      <td>17236.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.520</td>\n",
       "      <td>23.217</td>\n",
       "      <td>1.486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.124999e-02</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-02-28</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4925</td>\n",
       "      <td>7.546875</td>\n",
       "      <td>-0.026210</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>012994</td>\n",
       "      <td>16784.25</td>\n",
       "      <td>...</td>\n",
       "      <td>8.520</td>\n",
       "      <td>23.217</td>\n",
       "      <td>1.486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-2.620967e-02</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-03-31</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4925</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>-0.032258</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>-0.002444</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>012994</td>\n",
       "      <td>16830.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.108</td>\n",
       "      <td>20.823</td>\n",
       "      <td>1.829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.970750e-03</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-04-30</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4925</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>012994</td>\n",
       "      <td>16830.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.108</td>\n",
       "      <td>20.823</td>\n",
       "      <td>1.829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.138990e-09</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-05-31</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4925</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.018755</td>\n",
       "      <td>012994</td>\n",
       "      <td>17671.50</td>\n",
       "      <td>...</td>\n",
       "      <td>6.108</td>\n",
       "      <td>20.823</td>\n",
       "      <td>1.829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.19</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   share_code  exchange_code   sic       prc  ret_daily  \\\n",
       "permno date                                                               \n",
       "10001  1995-01-31          11              3  4925  7.750000   0.026915   \n",
       "       1995-02-28          11              3  4925  7.546875  -0.026210   \n",
       "       1995-03-31          11              3  4925  7.500000  -0.032258   \n",
       "       1995-04-30          11              3  4925  7.500000  -0.006211   \n",
       "       1995-05-31          11              3  4925  7.875000   0.000000   \n",
       "\n",
       "                   shrout    vwretd    sprtrn   gvkey   mkt_cap  ...   lctq  \\\n",
       "permno date                                                      ...          \n",
       "10001  1995-01-31  2224.0  0.003962  0.004077  012994  17236.00  ...  8.520   \n",
       "       1995-02-28  2224.0  0.008116  0.007400  012994  16784.25  ...  8.520   \n",
       "       1995-03-31  2244.0 -0.002444 -0.003007  012994  16830.00  ...  6.108   \n",
       "       1995-04-30  2244.0  0.001800  0.002259  012994  16830.00  ...  6.108   \n",
       "       1995-05-31  2244.0  0.014017  0.018755  012994  17671.50  ...  6.108   \n",
       "\n",
       "                      ltq  oiadpq pstkq   saleq  oancfy  dvpspq  prccq  \\\n",
       "permno date                                                              \n",
       "10001  1995-01-31  23.217   1.486   0.0  10.537     NaN    0.00    8.0   \n",
       "       1995-02-28  23.217   1.486   0.0  10.537     NaN    0.00    8.0   \n",
       "       1995-03-31  20.823   1.829   0.0  11.266     NaN    0.19    7.5   \n",
       "       1995-04-30  20.823   1.829   0.0  11.266     NaN    0.19    7.5   \n",
       "       1995-05-31  20.823   1.829   0.0  11.266     NaN    0.19    7.5   \n",
       "\n",
       "                    ret_monthly   industry  \n",
       "permno date                                 \n",
       "10001  1995-01-31 -3.124999e-02  Utilities  \n",
       "       1995-02-28 -2.620967e-02  Utilities  \n",
       "       1995-03-31  5.970750e-03  Utilities  \n",
       "       1995-04-30  8.138990e-09  Utilities  \n",
       "       1995-05-31  5.000000e-02  Utilities  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfb6ef",
   "metadata": {},
   "source": [
    "### 3. Create Style Factor Descriptors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf87171",
   "metadata": {},
   "source": [
    "#### 3.1 Size and Value Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74643eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating raw style factor descriptors...\n",
      "  Calculating Value descriptors...\n"
     ]
    }
   ],
   "source": [
    "# --- Creating Style Factor Descriptors ---\n",
    "\n",
    "print(\"Calculating raw style factor descriptors...\")\n",
    "\n",
    "# --- Step 1: Ensure DataFrame is \"flat\" for calculations ---\n",
    "if isinstance(df.index, pd.MultiIndex):\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "# --- Size Descriptor ---\n",
    "df['size_desc'] = np.log(df['mkt_cap'])\n",
    "\n",
    "\n",
    "# --- Value Descriptors (Composite) ---\n",
    "print(\"  Calculating Value descriptors...\")\n",
    "# Descriptor 1: Book-to-Market (B/M)\n",
    "# We need a lagged version of book equity (ceqq) \n",
    "# because an attribute must be known when running the FM regression\n",
    "# First, let's go back to quarterly data for book equity\n",
    "fundamentals_for_bm = df[['permno', 'datadate', 'rdq', 'ceqq']].copy().drop_duplicates()\n",
    "fundamentals_for_bm.dropna(subset=['datadate'], inplace=True)\n",
    "\n",
    "# Use 'rdq' as the true announcement date, with a fallback on a simple offset\n",
    "fundamentals_for_bm['announcement_date'] = fundamentals_for_bm['rdq'].fillna(\n",
    "    fundamentals_for_bm['datadate'] + pd.DateOffset(months=6) # Use a 6-month lag for B/M\n",
    ")\n",
    "\n",
    "# Rename the 'ceqq' column to give it a descriptive, unique name BEFORE the merge.\n",
    "fundamentals_for_bm.rename(columns={'ceqq': 'book_equity_lagged'}, inplace=True)\n",
    "\n",
    "# Merge this lagged book equity back into the main panel\n",
    "df = pd.merge_asof(\n",
    "    left=df.sort_values('date'),\n",
    "    right=fundamentals_for_bm[['permno', 'announcement_date', 'book_equity_lagged']].sort_values('announcement_date'),\n",
    "    left_on='date',\n",
    "    right_on='announcement_date',\n",
    "    by='permno'\n",
    ")\n",
    "# Defining the Book-to-Market descriptor as the ratio of lagged book equity to market cap.\n",
    "df['bm_desc'] = df['book_equity_lagged'] / df['mkt_cap']\n",
    "\n",
    "# Descriptor 2: Earnings-to-Price (E/P)\n",
    "quarterly_fundamentals = df[['permno', 'datadate', 'rdq', 'ibq']].copy().drop_duplicates()\n",
    "quarterly_fundamentals.dropna(subset=['datadate'], inplace=True)\n",
    "quarterly_fundamentals.sort_values(by=['permno', 'datadate'], inplace=True)\n",
    "# defining last-12-months earnings \n",
    "quarterly_fundamentals['ltm_earnings'] = quarterly_fundamentals.groupby('permno')['ibq'].rolling(window=4, min_periods=4).sum().values\n",
    "# Use the actual report date 'rdq' as the announcement date.\n",
    "# We must handle cases where 'rdq' might be missing. If it is, offset the datadate forward by 3 months.\n",
    "quarterly_fundamentals['announcement_date'] = quarterly_fundamentals['rdq'].fillna(\n",
    "    quarterly_fundamentals['datadate'] + pd.DateOffset(months=3)\n",
    "    )\n",
    "#drop any rows with missing permno, announcement date or ltm earnings\n",
    "quarterly_fundamentals.dropna(subset=['permno', 'announcement_date', 'ltm_earnings'], inplace=True)\n",
    "\n",
    "df = pd.merge_asof(\n",
    "    left=df.sort_values('date'),\n",
    "    right=quarterly_fundamentals[['permno', 'announcement_date', 'ltm_earnings']].sort_values('announcement_date'),\n",
    "    left_on='date',\n",
    "    right_on='announcement_date',\n",
    "    by='permno'\n",
    ")\n",
    "df['ep_desc'] = df['ltm_earnings'] / df['mkt_cap']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106bb36c",
   "metadata": {},
   "source": [
    "#### 3.2 Momentum and Financial Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a5a1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating Momentum descriptors...\n",
      "  Calculating Financial Constraint (WW) descriptor...\n",
      "\n",
      "Raw style factor descriptors calculated successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Momentum Descriptors (Composite) ---\n",
    "# We use two descriptors for momentum: return from month t-12 to t-1 and return from t-6 t t-1 \n",
    "print(\"  Calculating Momentum descriptors...\")\n",
    "df['mom12_1_desc'] = df.sort_values('date').groupby('permno')['ret_monthly'].transform(lambda x: x.shift(1).rolling(11).apply(lambda r: (1+r).prod()-1))\n",
    "df['mom6_1_desc'] = df.sort_values('date').groupby('permno')['ret_monthly'].transform(lambda x: x.shift(1).rolling(5).apply(lambda r: (1+r).prod()-1))\n",
    "\n",
    "\n",
    "# --- Financial Constraints (Whited-Wu) ---\n",
    "# As a placeholer, we calculate the WW index for financial constraints. \n",
    "# This index can be complemented or repalced by a measure from textual analyses. \n",
    "print(\"  Calculating Financial Constraint (WW) descriptor...\")\n",
    "# cash flow over asset\n",
    "df['cf_at'] = (df['ibq'] + df['dpq']) / df['atq']\n",
    "# Dividend payment indicator\n",
    "df['div_pos'] = ((df['dvpspq'] * df['cshoq']) > 0).astype(int)\n",
    "# Leverage\n",
    "df['tLtd_at'] = df['dlttq'] / df['atq']\n",
    "# sales growth\n",
    "df['sg'] = df.sort_values('date').groupby('permno')['saleq'].pct_change(fill_method=None)\n",
    "# Lagged industry sales growth\n",
    "df['isg_industry'] = df.groupby(['industry', 'date'])['sg'].transform('mean')\n",
    "df['isg'] = df.groupby(['permno'])['isg_industry'].shift(1)\n",
    "# Calculating WW\n",
    "df['ww_desc'] = -0.091*df['cf_at'] - 0.062*df['div_pos'] + 0.021*df['tLtd_at'] - 0.044*np.log(df['atq'].replace(0, np.nan)) + 0.102*df['isg'] - 0.035*df['sg']\n",
    "\n",
    "df.drop(['cf_at', 'div_pos', 'tLtd_at', 'sg', 'isg_industry', 'isg'], axis = 1 , inplace = True)\n",
    "\n",
    "# --- Final Cleanup ---\n",
    "descriptor_cols = ['size_desc', 'bm_desc', 'ep_desc', 'mom12_1_desc', 'mom6_1_desc', 'ww_desc']\n",
    "for col in descriptor_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# --- Step 2: Set index back for the next steps ---\n",
    "df.set_index(['permno', 'date'], inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"\\nRaw style factor descriptors calculated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3c76a",
   "metadata": {},
   "source": [
    "### 4. Standardization and Composite Factor Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f33e8a",
   "metadata": {},
   "source": [
    "#### 4.1 Creating the standardization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "201334b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_cap_weighted(series, weights):\n",
    "    \"\"\"\n",
    "    Performs capitalization-weighted standardization on a single Series.\n",
    "    Handles NaN values\n",
    "    \"\"\"\n",
    "    # --- Guard Clause: Check for all-NaN input ---\n",
    "    # If the series has no valid data points, we can't standardize. Return NaNs.\n",
    "    if series.isnull().all():\n",
    "        return pd.Series(np.nan, index=series.index)\n",
    "\n",
    "    # Ensure indices match and align the data\n",
    "    series, weights = series.align(weights, join='left')\n",
    "    \n",
    "    # Identify the valid (non-NaN) data points\n",
    "    is_valid = series.notna()\n",
    "    \n",
    "    # Calculate the sum of weights for valid data points\n",
    "    valid_weights_sum = weights[is_valid].sum()\n",
    "    \n",
    "    # --- Guard Clause: Check for zero valid weights ---\n",
    "    if valid_weights_sum == 0:\n",
    "        return pd.Series(np.nan, index=series.index)\n",
    "        \n",
    "    # Calculate weighted mean using only valid data\n",
    "    mean = np.sum(series[is_valid] * weights[is_valid]) / valid_weights_sum\n",
    "\n",
    "    # Calculate weighted standard deviation\n",
    "    de_meaned = series - mean\n",
    "    weighted_var = np.sum((de_meaned[is_valid]**2) * weights[is_valid]) / valid_weights_sum\n",
    "    std_dev = np.sqrt(weighted_var)\n",
    "\n",
    "    # --- Guard Clause: Check for zero standard deviation ---\n",
    "    # If all valid values are the same, std_dev will be 0. Return 0 for all.\n",
    "    if std_dev == 0:\n",
    "        return pd.Series(0.0, index=series.index)\n",
    "\n",
    "    # Calculate and return the Z-scores\n",
    "    return de_meaned / std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152f387",
   "metadata": {},
   "source": [
    "####  4.2 Applying the standardization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9645545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing all raw descriptors...\n",
      "Standardization complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate cap weights for each month\n",
    "df['cap_weight'] = df.groupby('date')['mkt_cap'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# List of our raw descriptor columns\n",
    "descriptor_cols = ['size_desc', 'bm_desc', 'ep_desc', 'mom12_1_desc', 'mom6_1_desc', 'ww_desc']\n",
    "\n",
    "# --- Standardize ALL descriptors month-by-month using transform ---\n",
    "print(\"Standardizing all raw descriptors...\")\n",
    "for col in descriptor_cols:\n",
    "    new_col_name = f\"z_{col}\"\n",
    "    # The transform will apply our function to each 'date' group\n",
    "    df[new_col_name] = df.groupby('date')[col].transform(\n",
    "        lambda x: standardize_cap_weighted(x, df.loc[x.index, 'cap_weight'])\n",
    "    )\n",
    "\n",
    "print(\"Standardization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cb3971",
   "metadata": {},
   "source": [
    "#### 4.3 Build and Re-Standardize Composite Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59d6a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Composite factors created. Now re-standardizing...\n",
      "Final factors assembled and cleaned.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the composite factors by averaging the standardized descriptors\n",
    "df['Value_composite'] = df[['z_bm_desc', 'z_ep_desc']].mean(axis=1)\n",
    "df['Momentum_composite'] = df[['z_mom12_1_desc', 'z_mom6_1_desc']].mean(axis=1)\n",
    "\n",
    "print(\"\\nComposite factors created. Now re-standardizing...\")\n",
    "\n",
    "# Re-standardize the final composites\n",
    "# This ensures they have a clean mean=0, std=1 profile\n",
    "df['Value'] = df.groupby('date')['Value_composite'].transform(\n",
    "    lambda x: standardize_cap_weighted(x, df.loc[x.index, 'cap_weight'])\n",
    ")\n",
    "df['Momentum'] = df.groupby('date')['Momentum_composite'].transform(\n",
    "    lambda x: standardize_cap_weighted(x, df.loc[x.index, 'cap_weight'])\n",
    ")\n",
    "\n",
    "# Rename the single-descriptor factors for consistency\n",
    "df.rename(columns={\n",
    "    'z_size_desc': 'Size',\n",
    "    'z_ww_desc': 'FinConstraint'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Final factors assembled and cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEMP_FILE = os.path.join(onedrive_root, \"0. DATASETS\", \"temps\", \"df_temp.parquet\")\n",
    "#df.to_parquet(TEMP_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff91f4",
   "metadata": {},
   "source": [
    "### 5. Assemble and Save the Final X Matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33091bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_dummies = pd.get_dummies(df['industry'], prefix='Ind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b531628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final time-varying factor exposure matrix (X) saved to D:\\OneDrive\\0. DATASETS\\outputs\\factor_exposures.parquet\n",
      "Shape of final X matrix: (1030177, 16)\n",
      "Notebook 2 (Project Titan) is complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Our final style factors are the re-standardized composites and the single descriptors\n",
    "style_factors = ['Size', 'Value', 'Momentum', 'FinConstraint']\n",
    "\n",
    "# Combine our final style factors with the industry dummies\n",
    "# First, let's align them to the same index (date, permno)\n",
    "df_for_x = df[style_factors].copy()\n",
    "X = df_for_x.join(industry_dummies)\n",
    "\n",
    "# Drop any rows with missing factor exposures, as we can't use them in the regression\n",
    "X.dropna(inplace=True)\n",
    "\n",
    "# Define the output file path\n",
    "X_FILE = os.path.join(DATA_DIR, 'factor_exposures.parquet')\n",
    "\n",
    "# Save the final, time-varying X matrix to a Parquet file\n",
    "X.to_parquet(X_FILE)\n",
    "\n",
    "print(f\"\\nFinal time-varying factor exposure matrix (X) saved to {X_FILE}\")\n",
    "print(f\"Shape of final X matrix: {X.shape}\")\n",
    "print(\"Notebook 2 (Project Titan) is complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

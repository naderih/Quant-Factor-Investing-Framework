{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb97f3ba",
   "metadata": {},
   "source": [
    "# Project Titan - Notebook 1: Data Engineering & CRSP-Compustat Merge\n",
    "\n",
    "### **Objective**\n",
    "This notebook constructs the foundational dataset for our quantitative analysis. It takes the pre-filtered, large-scale datasets from CRSP and Compustat (prepared offline in Stata for efficiency) and performs the crucial **CRSP-Compustat Merge (CCM)**. The goal is to produce a single, clean, \"point-in-time\" monthly panel dataset that correctly aligns market data (like returns) with the appropriate lagged fundamental data from financial statements.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methodology: The Point-in-Time Merge**\n",
    "\n",
    "The core challenge in building a research-quality dataset is the correct temporal alignment of data from different sources. Market data (CRSP) is available daily, while fundamental data (Compustat) is reported quarterly with a significant lag. A naive merge would introduce severe **lookahead bias**.\n",
    "\n",
    "The methodology applied here follows the standard academic and professional approach:\n",
    "\n",
    "*   **1. Link File Preparation:** First load the clean CCM linking table, which provides the historical mapping between CRSP's `PERMNO` and Compustat's `GVKEY`.\n",
    "\n",
    "*   **2. Event-Driven Merge (`merge_asof`):** We use Python's `pandas` library, specifically the powerful `pd.merge_asof()` function. This function performs a \"nearest-in-time\" merge, which is the correct tool for this problem. The process is a two-step merge:\n",
    "    *   First, we merge the CRSP market data with the CCM linking table to assign the correct `GVKEY` to each `PERMNO` for each point in time.\n",
    "    *   Second, we merge this combined market data with the Compustat fundamental data. The `merge_asof` ensures that for any given month's market data, we are merging it with the **most recently available public financial statement data**, correctly simulating the information delay.\n",
    "\n",
    "*   **3. Feature Engineering & Finalization:** Once the panel is constructed, we calculate key derived variables (e.g., market capitalization, excess returns) and resample the data to a monthly frequency.\n",
    "\n",
    "---\n",
    "**Output:** This notebook's final output is a single, analysis-ready `panel_data.parquet` file. Parquet format is highly efficient for storing large, structured datasets, preserving data types and offering significant speed advantages over CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c2ba5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "onedrive_root = str(Path(os.environ['OneDrive']))\n",
    "INPUT_DATA_DIR = os.path.join(onedrive_root, \"0. DATASETS\", \"temps\")\n",
    "\n",
    "# --- Define File Paths ---\n",
    "CRSP_FILE = os.path.join(INPUT_DATA_DIR, 'crsp_clean_daily.dta')\n",
    "COMP_FILE = os.path.join(INPUT_DATA_DIR, 'compustat_clean_quarterly.dta')\n",
    "CCM_FILE = os.path.join(INPUT_DATA_DIR, 'ccm_linking_table_clean.dta')\n",
    "OUTPUT_FILE = os.path.join(onedrive_root, \"0. DATASETS\", \"outputs\", 'panel_data.parquet')\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d4b17",
   "metadata": {},
   "source": [
    "### 2. Load the Three Cleaned CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27552c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Converting date columns in crsp to datetime objects...\n",
      "CRSP, Compustat, and CCM data loaded successfully.\n",
      "CRSP shape: (35166396, 10)\n",
      "Compustat shape: (605175, 22)\n"
     ]
    }
   ],
   "source": [
    "# Load crsp data \n",
    "crsp = pd.read_stata(CRSP_FILE)\n",
    "crsp.rename(columns= {'ret' : 'ret_daily'}, inplace = True) \n",
    "print(\"\\n Converting date columns in crsp to datetime objects...\")\n",
    "crsp['date'] = pd.to_datetime(crsp['date'],  format = '%d%b%Y')\n",
    "\n",
    "# Load Compustat Data\n",
    "comp = pd.read_stata(COMP_FILE)\n",
    "# drop the header sic from compustat as we have the historical value from crsp\n",
    "comp.drop('sic', axis = 1, inplace = True)\n",
    "\n",
    "# Load CCM Linking Table\n",
    "ccm = pd.read_stata(CCM_FILE)\n",
    "\n",
    "print(\"CRSP, Compustat, and CCM data loaded successfully.\")\n",
    "print(f\"CRSP shape: {crsp.shape}\")\n",
    "print(f\"Compustat shape: {comp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26146db9",
   "metadata": {},
   "source": [
    "### 3. The CRSP-CCM Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b84e3376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRSP and CCM merged successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1 of the Merge: Link CRSP and CCM ---\n",
    "\n",
    "# Sort both tables by the linking key (permno) and the date\n",
    "crsp.sort_values(by=['date', 'permno'], inplace=True)\n",
    "ccm.sort_values(by=['link_start_date', 'permno' ], inplace=True)\n",
    "\n",
    "# After sorting, we reset the index. This ensures the DataFrame's internal\n",
    "# structure is clean and satisfies the strict sorting requirement of merge_asof.\n",
    "crsp.reset_index(drop=True, inplace=True)\n",
    "ccm.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# nearest merge to crsp going backwards in time\n",
    "# Perform the backward merge_asof to find the correct link for each CRSP observation\n",
    "# merge_asof: for each row in left (crsp date), find the nearest earlier matching row in right (ccm link start).\n",
    "# This finds the most recent link that was active as of the CRSP date.\n",
    "# => looks in ccm for the most recent link_start_date that is â‰¤  CRSP date, and attaches that row.\n",
    "\n",
    "crsp_ccm = pd.merge_asof(left=crsp,\n",
    "                         right=ccm,\n",
    "                         left_on='date',\n",
    "                         right_on='link_start_date',\n",
    "                         by='permno'\n",
    "                         )\n",
    "\n",
    "# the above matches each crsp date to all link_start_dates. some of the linked matches ended prior to cris date. \n",
    "# => filter out any matches where the link was no longer valid\n",
    "# i.e., where the CRSP date is after the link's end date\n",
    "crsp_ccm = crsp_ccm[crsp_ccm['date'] <= crsp_ccm['link_end_date']]\n",
    "\n",
    "# calculate market capitalization as number of shares outstanding x share price\n",
    "crsp_ccm['mkt_cap'] = crsp_ccm['prc'] * crsp_ccm['shrout']\n",
    "print(\"CRSP and CCM merged successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a4858",
   "metadata": {},
   "source": [
    "### 4. Merge with Compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddef127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full daily panel data constructed.\n",
      "Daily panel shape: (34640540, 33)\n"
     ]
    }
   ],
   "source": [
    "# sort by date and the linking key (gvkey)\n",
    "crsp_ccm.sort_values(by = ['date', 'gvkey'], inplace = True)\n",
    "comp.sort_values(by = ['datadate', 'gvkey'], inplace = True)\n",
    "\n",
    "panel_data = pd.merge_asof(left = crsp_ccm, \n",
    "                           right = comp, \n",
    "                           left_on = 'date', \n",
    "                           right_on = 'datadate', \n",
    "                           by = 'gvkey')\n",
    "\n",
    "# drop the helper columns\n",
    "panel_data.drop(columns=['link_start_date', 'link_end_date'], inplace=True)\n",
    "\n",
    "print(\"Full daily panel data constructed.\")\n",
    "print(f\"Daily panel shape: {panel_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11910d4",
   "metadata": {},
   "source": [
    "### 5. Resample to Monthly and Calculate Monthly Returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430716b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will group by each firm (permno) and then by month\n",
    "# and take the last observation of the month.\n",
    "# This gives us month-end values for market cap, fundamentals, etc.\n",
    "\n",
    "monthly_panel = panel_data.groupby('permno').resample('ME', on = 'date').last()\n",
    "\n",
    "# Dropping the extra 'permno' column from the index\n",
    "monthly_panel.reset_index(level = 0 , drop = True, inplace = True)\n",
    "\n",
    "# Calculate monthly returns by compounding the daily returns within each month\n",
    "# 1+r_month = (1+r_1)(1+r_2)...(1+r_30)\n",
    "monthly_returns = panel_data.groupby(['permno', pd.Grouper(key='date', freq='ME')])['ret_daily'].apply(lambda x: (1 + x).prod() - 1)\n",
    "\n",
    "# turning it to dataframe and neming the ret_monthly column \n",
    "monthly_returns = monthly_returns.to_frame(name='ret_monthly')\n",
    "\n",
    "# Merge the monthly returns back into our main panel\n",
    "# join takes column from the left df and pairs with index of the right df \n",
    "monthly_panel = monthly_panel.join(monthly_returns, on=['permno', 'date'])\n",
    "\n",
    "# Forward-fill any fundamentals that might be missing for a month\n",
    "# makea list of fundamentals columns to be ffilled\n",
    "ffill_cols = comp.columns.drop(['datadate', 'gvkey'])\n",
    "monthly_panel[ffill_cols] = monthly_panel.groupby('permno')[ffill_cols].ffill()\n",
    "\n",
    "# any entry with missing market cap or monthly returns will be dropped\n",
    "monthly_panel.dropna(subset=['ret_monthly', 'mkt_cap'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02a487",
   "metadata": {},
   "source": [
    "### 6. Save the Final Panel Datase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b93e71e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged panel data saved to D:\\OneDrive\\0. DATASETS\\outputs\\panel_data.parquet\n",
      "Notebook 1 (Project Titan) is complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Final Dataset in Parquet Format ---\n",
    "\n",
    "monthly_panel.to_parquet(OUTPUT_FILE)\n",
    "\n",
    "print(f\"Final merged panel data saved to {OUTPUT_FILE}\")\n",
    "print(\"Notebook 1 (Project Titan) is complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

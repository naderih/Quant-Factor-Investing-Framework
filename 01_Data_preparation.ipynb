{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb97f3ba",
   "metadata": {},
   "source": [
    "# Project Titan - Notebook 1: Data Engineering & CRSP-Compustat Merge\n",
    "\n",
    "### **Objective**\n",
    "This notebook constructs the foundational dataset for our quantitative analysis. It takes the pre-filtered, large-scale datasets from CRSP and Compustat (prepared offline in Stata for efficiency) and performs the crucial **CRSP-Compustat Merge (CCM)**. The goal is to produce a single, clean, \"point-in-time\" monthly panel dataset that correctly aligns market data (like returns) with the appropriate lagged fundamental data from financial statements.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methodology: The Point-in-Time Merge**\n",
    "\n",
    "The core challenge in building a research-quality dataset is the correct temporal alignment of data from different sources. Market data (CRSP) is available daily, while fundamental data (Compustat) is reported quarterly with a significant lag. A naive merge would introduce severe **lookahead bias**.\n",
    "\n",
    "The methodology applied here follows the standard academic and professional approach:\n",
    "\n",
    "*   **1. Link File Preparation:** First load the clean CCM linking table, which provides the historical mapping between CRSP's `PERMNO` and Compustat's `GVKEY`.\n",
    "\n",
    "*   **2. Event-Driven Merge (`merge_asof`):** We use Python's `pandas` library, specifically the powerful `pd.merge_asof()` function. This function performs a \"nearest-in-time\" merge, which is the correct tool for this problem. The process is a two-step merge:\n",
    "    *   First, we merge the CRSP market data with the CCM linking table to assign the correct `GVKEY` to each `PERMNO` for each point in time.\n",
    "    *   Second, we merge this combined market data with the Compustat fundamental data. The `merge_asof` ensures that for any given month's market data, we are merging it with the **most recently available public financial statement data**, correctly simulating the information delay.\n",
    "\n",
    "*   **3. Feature Engineering & Finalization:** Once the panel is constructed, we calculate key derived variables (e.g., market capitalization, excess returns) and resample the data to a monthly frequency.\n",
    "\n",
    "---\n",
    "**Output:** This notebook's final output is a single, analysis-ready `panel_data.parquet` file. Parquet format is highly efficient for storing large, structured datasets, preserving data types and offering significant speed advantages over CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2ba5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "onedrive_root = str(Path(os.environ['OneDrive']))\n",
    "INPUT_DATA_DIR = os.path.join(onedrive_root, \"0. DATASETS\", \"temps\")\n",
    "\n",
    "# --- Define File Paths ---\n",
    "CRSP_FILE = os.path.join(INPUT_DATA_DIR, 'crsp_clean_daily.dta')\n",
    "COMP_FILE = os.path.join(INPUT_DATA_DIR, 'compustat_clean_quarterly.dta')\n",
    "CCM_FILE = os.path.join(INPUT_DATA_DIR, 'ccm_linking_table_clean.dta')\n",
    "OUTPUT_FILE = os.path.join('data', 'panel_data.parquet')\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1d4b17",
   "metadata": {},
   "source": [
    "### 2. Load the Three Cleaned CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27552c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Converting date columns in crsp to datetime objects...\n",
      "CRSP, Compustat, and CCM data loaded successfully.\n",
      "CRSP shape: (35166396, 10)\n",
      "Compustat shape: (605175, 23)\n"
     ]
    }
   ],
   "source": [
    "# Load crsp data \n",
    "crsp = pd.read_stata(CRSP_FILE)\n",
    "crsp.rename(columns= {'ret' : 'ret_daily'}, inplace = True) \n",
    "print(\"\\n Converting date columns in crsp to datetime objects...\")\n",
    "crsp['date'] = pd.to_datetime(crsp['date'],  format = '%d%b%Y')\n",
    "\n",
    "# Load Compustat Data\n",
    "comp = pd.read_stata(COMP_FILE)\n",
    "\n",
    "# Load CCM Linking Table\n",
    "ccm = pd.read_stata(CCM_FILE)\n",
    "\n",
    "print(\"CRSP, Compustat, and CCM data loaded successfully.\")\n",
    "print(f\"CRSP shape: {crsp.shape}\")\n",
    "print(f\"Compustat shape: {comp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26146db9",
   "metadata": {},
   "source": [
    "### 3. The CRSP-CCM Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e3376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRSP and CCM merged successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1 of the Merge: Link CRSP and CCM ---\n",
    "\n",
    "# Sort both tables by the linking key (permno) and the date\n",
    "crsp.sort_values(by=['date', 'permno'], inplace=True)\n",
    "ccm.sort_values(by=['link_start_date', 'permno' ], inplace=True)\n",
    "\n",
    "# After sorting, we reset the index. This ensures the DataFrame's internal\n",
    "# structure is clean and satisfies the strict sorting requirement of merge_asof.\n",
    "crsp.reset_index(drop=True, inplace=True)\n",
    "ccm.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# nearest merge to crsp going backwards in time\n",
    "# Perform the backward merge_asof to find the correct link for each CRSP observation\n",
    "# merge_asof: for each row in left (crsp date), find the nearest earlier matching row in right (ccm link start).\n",
    "# This finds the most recent link that was active as of the CRSP date.\n",
    "# => looks in ccm for the most recent link_start_date that is â‰¤  CRSP date, and attaches that row.\n",
    "\n",
    "crsp_ccm = pd.merge_asof(left=crsp,\n",
    "                         right=ccm,\n",
    "                         left_on='date',\n",
    "                         right_on='link_start_date',\n",
    "                         by='permno'\n",
    "                         )\n",
    "\n",
    "# the above matches each crsp date to all link_start_dates. some of the linked matches ended prior to cris date. \n",
    "# => filter out any matches where the link was no longer valid\n",
    "# i.e., where the CRSP date is after the link's end date\n",
    "crsp_ccm = crsp_ccm[crsp_ccm['date'] <= crsp_ccm['link_end_date']]\n",
    "\n",
    "# calculate market capitalization as number of shares outstanding x share price\n",
    "crsp_ccm['mkt_cap'] = crsp_ccm['prc'] * crsp_ccm['shrout']\n",
    "print(\"CRSP and CCM merged successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a4858",
   "metadata": {},
   "source": [
    "### 4. Merge with Compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddef127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full daily panel data constructed.\n",
      "Daily panel shape: (34640540, 34)\n"
     ]
    }
   ],
   "source": [
    "# sort by date and the linking key (gvkey)\n",
    "crsp_ccm.sort_values(by = ['date', 'gvkey'], inplace = True)\n",
    "comp.sort_values(by = ['datadate', 'gvkey'], inplace = True)\n",
    "\n",
    "panel_data = pd.merge_asof(left = crsp_ccm, \n",
    "                           right = comp, \n",
    "                           left_on = 'date', \n",
    "                           right_on = 'datadate', \n",
    "                           by = 'gvkey')\n",
    "\n",
    "# drop the helper columns\n",
    "panel_data.drop(columns=['link_start_date', 'link_end_date'], inplace=True)\n",
    "\n",
    "print(\"Full daily panel data constructed.\")\n",
    "print(f\"Daily panel shape: {panel_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11910d4",
   "metadata": {},
   "source": [
    "### 5. Resample to Monthly and Calculate Monthly Returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430716b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will group by each firm (permno) and then by month\n",
    "# and take the last observation of the month.\n",
    "# This gives us month-end values for market cap, fundamentals, etc.\n",
    "\n",
    "monthly_panel = panel_data.groupby('permno').resample('ME', on = 'date').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02eab90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the extra 'permno' column from the index\n",
    "monthly_panel.reset_index(level = 0 , drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5389caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate monthly returns by compounding the daily returns within each month\n",
    "# 1+r_month = (1+r_1)(1+r_2)...(1+r_30)\n",
    "monthly_returns = panel_data.groupby(['permno', pd.Grouper(key='date', freq='ME')])['ret_daily'].apply(lambda x: (1 + x).prod() - 1)\n",
    "# turning it to dataframe and neming the ret_monthly column \n",
    "monthly_returns = monthly_returns.to_frame(name='ret_monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008920c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the monthly returns back into our main panel\n",
    "# join takes column from the left df and pairs with index of the right df \n",
    "monthly_panel = monthly_panel.join(monthly_returns, on=['permno', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68e2aa54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['permno', 'share_code', 'exchange_code', 'sic_x', 'prc', 'ret_daily',\n",
       "       'shrout', 'vwretd', 'sprtrn', 'gvkey', 'mkt_cap', 'datadate', 'fyearq',\n",
       "       'fqtr', 'tic', 'actq', 'atq', 'ceqq', 'cheq', 'cshoq', 'dlcq', 'dlttq',\n",
       "       'dpq', 'ibq', 'lctq', 'ltq', 'oiadpq', 'pstkq', 'saleq', 'oancfy',\n",
       "       'dvpspq', 'prccq', 'sic_y', 'ret_monthly'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_panel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e08c465",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Columns not found: 'sic'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Forward-fill any fundamentals that might be missing for a month\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# makea list of fundamentals columns to be ffilled\u001b[39;00m\n\u001b[32m      3\u001b[39m ffill_cols = comp.columns.drop([\u001b[33m'\u001b[39m\u001b[33mdatadate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgvkey\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m monthly_panel[ffill_cols] = \u001b[43mmonthly_panel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpermno\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mffill_cols\u001b[49m\u001b[43m]\u001b[49m.ffill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HoNad\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1951\u001b[39m, in \u001b[36mDataFrameGroupBy.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) > \u001b[32m1\u001b[39m:\n\u001b[32m   1945\u001b[39m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[32m   1946\u001b[39m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[32m   1947\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1948\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1949\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1950\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HoNad\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\base.py:239\u001b[39m, in \u001b[36mSelectionMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj.columns.intersection(key)) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(key)):\n\u001b[32m    238\u001b[39m         bad_keys = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(key).difference(\u001b[38;5;28mself\u001b[39m.obj.columns))\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(bad_keys)[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gotitem(\u001b[38;5;28mlist\u001b[39m(key), ndim=\u001b[32m2\u001b[39m)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: \"Columns not found: 'sic'\""
     ]
    }
   ],
   "source": [
    "# Forward-fill any fundamentals that might be missing for a month\n",
    "# makea list of fundamentals columns to be ffilled\n",
    "ffill_cols = comp.columns.drop(['datadate', 'gvkey'])\n",
    "monthly_panel[ffill_cols] = monthly_panel.groupby('permno')[ffill_cols].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a18b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any entry with missing market cap or monthly returns will be dropped\n",
    "monthly_panel.dropna(subset=['ret_monthly', 'mkt_cap'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e91df8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['permno', 'hsiccd', 'prc', 'vol', 'ret_daily', 'shrout', 'gvkey',\n",
       "       'mkt_cap', 'datadate', 'fyearq', 'fqtr', 'tic', 'actq', 'atq', 'ceqq',\n",
       "       'cheq', 'cshoq', 'dlcq', 'dlttq', 'dpq', 'ibq', 'lctq', 'ltq', 'oiadpq',\n",
       "       'pstkq', 'saleq', 'oancfy', 'dvpspq', 'prccq', 'sic', 'ret_monthly'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_panel.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02a487",
   "metadata": {},
   "source": [
    "### 6. Save the Final Panel Datase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b93e71e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged panel data saved to data\\panel_data.parquet\n",
      "Notebook 1 (Project Titan) is complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Final Dataset in Parquet Format ---\n",
    "\n",
    "monthly_panel.to_parquet(OUTPUT_FILE)\n",
    "\n",
    "print(f\"Final merged panel data saved to {OUTPUT_FILE}\")\n",
    "print(\"Notebook 1 (Project Titan) is complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
